---
title: "Data Manipulation Challenge"
subtitle: "A Mental Model for Method Chaining in Pandas"
format:
  html: default
  pdf: default
execute:
  echo: true
  eval: true
---

# 🔗 Data Manipulation Challenge - A Mental Model for Method Chaining in Pandas

::: {.callout-important}
## 📊 Challenge Requirements In Section [Student Analysis Section](#student-analysis-section)
- Complete all discussion questions for the seven mental models (plus some extra requirements for higher grades)
:::

::: {.callout-important}
## 🎯 Note on Python Usage

**Recommended Workflow: Use Your Existing Virtual Environment**
If you completed the Tech Setup Challenge Part 2, you already have a virtual environment set up! Here's how to use it for this new challenge:

1. **Clone this new challenge repository** (see Getting Started section below)
2. **Open the cloned repository in Cursor**
3. **Set this project to use your existing Python interpreter:**
   - Press `Ctrl+Shift+P` → "Python: Select Interpreter"
   - Navigate to and choose the interpreter from your existing virtual environment (e.g., `your-previous-project/venv/Scripts/python.exe`)
4. **Activate the environment in your terminal:**
   - Open terminal in Cursor (`Ctrl + ``)
   - Navigate to your previous project folder where you have the `venv` folder
   - **💡 Pro tip:** You can quickly navigate by typing `cd` followed by dragging the folder from your file explorer into the terminal
   - Activate using the appropriate command for your system:
     - **Windows Command Prompt:** `venv\Scripts\activate`
     - **Windows PowerShell:** `.\venv\Scripts\Activate.ps1`
     - **Mac/Linux:** `source venv/bin/activate`
   - You should see `(venv)` at the beginning of your terminal prompt
5. **Install additional packages if needed:** `pip install pandas numpy matplotlib seaborn`

::: {.callout-warning}
## ⚠️ Cloud Storage Warning

**Avoid using Google Drive, OneDrive, or other cloud storage for Python projects!** These services can cause issues with:
- Package installations failing due to file locking
- Virtual environment corruption
- Slow performance during pip operations

**Best practice:** Keep your Python projects in a local folder like `C:\Users\YourName\Documents\` or `~/Documents/` instead of cloud-synced folders.
:::

**Alternative: Create a New Virtual Environment**
If you prefer a fresh environment, follow the Quarto documentation: [https://quarto.org/docs/projects/virtual-environments.html](https://quarto.org/docs/projects/virtual-environments.html). Be sure to follow the instructions to activate the environment, set it up as your default Python interpreter for the project, and install the necessary packages (e.g. pandas) for this challenge.  For installing the packages, you can use the `pip install -r requirements.txt` command since you already have the requirements.txt file in your project.   Some steps do take a bit of time, so be patient.

**Why This Works:** Virtual environments are portable - you can use the same environment across multiple projects, and Cursor automatically activates it when you select the interpreter!

:::

## The Problem: Mastering Data Manipulation Through Method Chaining

**Core Question:** How can we efficiently manipulate datasets using `pandas` method chaining to answer complex business questions?

**The Challenge:** Real-world data analysis requires combining multiple data manipulation techniques in sequence. Rather than creating intermediate variables at each step, method chaining allows us to write clean, readable code that flows logically from one operation to the next.

**Our Approach:** We'll work with ZappTech's shipment data to answer critical business questions about service levels and cross-category orders, using the seven mental models of data manipulation through pandas method chaining.

::: {.callout-warning}
## ⚠️ AI Partnership Required

This challenge pushes boundaries intentionally. You'll tackle problems that normally require weeks of study, but with Cursor AI as your partner (and your brain keeping it honest), you can accomplish more than you thought possible.

**The new reality:** The four stages of competence are Ignorance → Awareness → Learning → Mastery. AI lets us produce Mastery-level work while operating primarily in the Awareness stage. I focus on awareness training, you leverage AI for execution, and together we create outputs that used to require years of dedicated study.
:::

## The Seven Mental Models of Data Manipulation

The seven most important ways we manipulate datasets are:

1. **Assign:** Add new variables with calculations and transformations
2. **Subset:** Filter data based on conditions or select specific columns
3. **Drop:** Remove unwanted variables or observations
4. **Sort:** Arrange data by values or indices
5. **Aggregate:** Summarize data using functions like mean, sum, count
6. **Merge:** Combine information from multiple datasets
7. **Split-Apply-Combine:** Group data and apply functions within groups


## Data and Business Context

We analyze ZappTech's shipment data, which contains information about product deliveries across multiple categories. This dataset is ideal for our analysis because:

- **Real Business Questions:** CEO wants to understand service levels and cross-category shopping patterns
- **Multiple Data Sources:** Requires merging shipment data with product category information
- **Complex Relationships:** Service levels may vary by product category, and customers may order across categories
- **Method Chaining Practice:** Perfect for demonstrating all seven mental models in sequence

## Data Loading and Initial Exploration

Let's start by loading the ZappTech shipment data and understanding what we're working with.

```{python}
#| label: load-data
#| echo: true
#| message: false
#| warning: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta

# Load the shipment data
shipments_df = pd.read_csv(
    "https://raw.githubusercontent.com/flyaflya/persuasive/main/shipments.csv", 
    parse_dates=['plannedShipDate', 'actualShipDate']
)

# Load product line data
product_line_df = pd.read_csv(
    "https://raw.githubusercontent.com/flyaflya/persuasive/main/productLine.csv"
)

# Reduce dataset size for faster processing (4,000 rows instead of 96,805 rows)
shipments_df = shipments_df.head(4000)

print("Shipments data shape:", shipments_df.shape)
print("\nShipments data columns:", shipments_df.columns.tolist())
print("\nFirst few rows of shipments data:")
print(shipments_df.head(10))

print("\n" + "="*50)
print("Product line data shape:", product_line_df.shape)
print("\nProduct line data columns:", product_line_df.columns.tolist())
print("\nFirst few rows of product line data:")
print(product_line_df.head(10))
```

::: {.callout-note}
## 💡 Understanding the Data

**Shipments Data:** Contains individual line items for each shipment, including:
- `shipID`: Unique identifier for each shipment
- `partID`: Product identifier
- `plannedShipDate`: When the shipment was supposed to go out
- `actualShipDate`: When it actually shipped
- `quantity`: How many units were shipped

**Product Category and Line Data:** Contains product category information:
- `partID`: Links to shipments data
- `productLine`: The category each product belongs to
- `prodCategory`: The category each product belongs to

**Business Questions We'll Answer:**
1. Does service level (on-time shipments) vary across product categories?
2. How often do orders include products from more than one category?
:::

## The Seven Mental Models: A Progressive Learning Journey

Now we'll work through each of the seven mental models using method chaining, starting simple and building complexity.

### 1. Assign: Adding New Variables

**Mental Model:** Create new columns with calculations and transformations.

Let's start by calculating whether each shipment was late:

```{python}
#| label: mental-model-1-assign
#| echo: true

# Simple assignment - calculate if shipment was late
shipments_with_lateness = (
    shipments_df
    .assign(
        is_late=lambda df: df['actualShipDate'] > df['plannedShipDate'],
        days_late=lambda df: (df['actualShipDate'] - df['plannedShipDate']).dt.days
    )
)

print("Added lateness calculations:")
print(shipments_with_lateness[['shipID', 'plannedShipDate', 'actualShipDate', 'is_late', 'days_late']].head())
```

::: {.callout-tip}
## 💡 Method Chaining Tip for New Python Users

**Why use `lambda df:`?** When chaining methods, we need to reference the current state of the dataframe. The `lambda df:` tells pandas "use the current dataframe in this calculation." Without it, pandas would look for a variable called `df` that doesn't exist.

**Alternative approach:** You could also write this as separate steps, but method chaining keeps related operations together and makes the code more readable.
:::

::: {.callout-important}
## 🤔 Discussion Questions: Assign Mental Model

**Question 1: Data Types and Date Handling**
- What is the `dtype` of the `actualShipDate` series? How can you find out using code?
- Why is it important that both `actualShipDate` and `plannedShipDate` have the same data type for comparison?

**Question 2: String vs Date Comparison**
- Can you give an example where comparing two dates as strings would yield unintuitive results, e.g. what happens if you try to compare "04-11-2025" and "05-20-2024" as strings vs as dates?

**Question 3: Debug This Code**
```python
# This code has an error - can you spot it?
shipments_with_lateness = (
    shipments_df
    .assign(
        is_late=lambda df: df['actualShipDate'] > df['plannedShipDate'],
        days_late=lambda df: (df['actualShipDate'] - df['plannedShipDate']).dt.days,
        lateStatement="Darn Shipment is Late" if shipments_df['is_late'] else "Shipment is on Time"
    )
)
```
What's wrong with the `lateStatement` assignment and how would you fix it?
:::

#### Briefly Give Answers to the Discussion Questions In This Section

**Question 1: Data Types and Date Handling**

- **Finding the dtype:** Use `shipments_df['actualShipDate'].dtype` to check the data type. Both `actualShipDate` and `plannedShipDate` should be `datetime64[ns]` type.
- **Why same data type matters:** When comparing dates, pandas requires both columns to be datetime objects. If one is a string and the other is datetime, the comparison will fail or produce unexpected results. Having the same data type ensures accurate chronological comparisons.

**Question 2: String vs Date Comparison**

String comparison uses lexicographic (alphabetical) ordering, which gives unintuitive results for dates. For example:
- String comparison: `"04-11-2025" > "05-20-2024"` returns `False` because "04" comes before "05" alphabetically
- Date comparison: `datetime(2025, 4, 11) > datetime(2024, 5, 20)` returns `True` because April 2025 is actually later than May 2024

The string approach incorrectly treats dates as text, leading to wrong chronological ordering.

**Question 3: Debug This Code**

The error is in the `lateStatement` assignment: `shipments_df['is_late']` doesn't exist yet because we're currently creating that column. The fix is to use `lambda df:` and `.map()`:

```python
lateStatement=lambda df: df['is_late'].map({True: "Darn Shipment is Late", False: "Shipment is on Time"})
```

This references the current dataframe state and applies the conditional logic element-wise to the boolean series.

### 2. Subset: Querying Rows and Filtering Columns

**Mental Model:** Query rows based on conditions and filter to keep specific columns.

Let's query for only late shipments and filter to keep the columns we need:

```{python}
#| label: mental-model-2-subset
#| echo: true

# Query rows for late shipments and filter to keep specific columns
late_shipments = (
    shipments_with_lateness
    .query('is_late == True')  # Query rows where is_late is True
    .filter(['shipID', 'partID', 'plannedShipDate', 'actualShipDate', 'days_late'])  # Filter to keep specific columns
)

print(f"Found {len(late_shipments)} late shipments out of {len(shipments_with_lateness)} total")
print("\nLate shipments sample:")
print(late_shipments.head())
```

::: {.callout-note}
## 🔍 Understanding the Methods

- **`.query()`**: Query rows based on conditions (like SQL WHERE clause)
- **`.filter()`**: Filter to keep specific columns by name
- **Alternative**: You could use `.loc[]` for more complex row querying, but `.query()` is often more readable
:::

::: {.callout-important}
## 🤔 Discussion Questions: Subset Mental Model

**Question 1: Query vs Boolean Indexing**
- What's the difference between using `.query('is_late == True')` and `[df['is_late'] == True]`?
- Which approach is more readable and why?

**Question 2: Additional Row Querying**
- Can you show an example of using a variable like `late_threshold` to query rows for shipments that are at least `late_threshold` days late, e.g. what if you wanted to query rows for shipments that are at least 5 days late?
:::

#### Briefly Give Answers to the Discussion Questions In This Section

**Question 1: Query vs Boolean Indexing**

The main differences between `.query('is_late == True')` and `[df['is_late'] == True]` are:

1. **Syntax approach**: `.query()` uses string-based SQL-like syntax, while boolean indexing uses Python boolean expressions
2. **Readability**: `.query()` is more readable for simple conditions and reads like natural language
3. **Variable handling**: `.query()` can reference Python variables using `@variable_name`, while boolean indexing uses variables directly
4. **Complexity**: Boolean indexing is more flexible for complex Python logic, while `.query()` excels at simple filtering conditions

**Question 2: Additional Row Querying with Variables**

Here's how to use a `late_threshold` variable to query for shipments that are at least 5 days late:

```python
# Using .query() with variable reference
late_threshold = 5
very_late_shipments = (
    shipments_with_lateness
    .query('days_late >= @late_threshold')  # @ symbol references the variable
    .filter(['shipID', 'partID', 'days_late', 'is_late'])
)

# Alternative using boolean indexing
very_late_shipments_alt = (
    shipments_with_lateness
    [shipments_with_lateness['days_late'] >= late_threshold]
    .filter(['shipID', 'partID', 'days_late', 'is_late'])
)
```

The key difference is that `.query()` requires the `@` symbol to reference Python variables, while boolean indexing uses the variable directly in the condition.

### 3. Drop: Removing Unwanted Data

**Mental Model:** Remove columns or rows you don't need.

Let's clean up our data by removing unnecessary columns:

```{python}
#| label: mental-model-3-drop
#| echo: true

# Create a cleaner dataset by dropping unnecessary columns
clean_shipments = (
    shipments_with_lateness
    .drop(columns=['quantity'])  # Drop quantity column (not needed for our analysis)
    .dropna(subset=['plannedShipDate', 'actualShipDate'])  # Remove rows with missing dates
)

print(f"Cleaned dataset: {len(clean_shipments)} rows, {len(clean_shipments.columns)} columns")
print("Remaining columns:", clean_shipments.columns.tolist())
```

::: {.callout-important}
## 🤔 Discussion Questions: Drop Mental Model

**Question 1: Drop vs Filter Strategies**
- What's the difference between `.drop(columns=['quantity'])` and `.filter()` with a list of columns you want to keep?
- When would you choose to drop columns vs filter to keep specific columns?

**Question 2: Handling Missing Data**
- What happens if you use `.dropna()` without specifying `subset`? How is this different from `.dropna(subset=['plannedShipDate', 'actualShipDate'])`?
- Why might you want to be selective about which columns to check for missing values?

:::

#### Briefly Give Answers to the Discussion Questions In This Section

**Question 1: Drop vs Filter Strategies**

The key differences between `.drop(columns=['quantity'])` and `.filter()` with columns to keep are:

1. **`.drop(columns=['quantity'])`:**
   - **Explicit removal**: You specify exactly what you want to remove
   - **Good for**: When you know exactly which columns are problematic/unnecessary
   - **Risk**: If you miss a column name, it stays in the dataset
   - **Use case**: Removing specific problematic columns from a large dataset

2. **`.filter()` with columns to keep:**
   - **Explicit inclusion**: You specify exactly what you want to keep
   - **Good for**: When you only need a few specific columns for analysis
   - **Safety**: Only the columns you explicitly list will remain
   - **Use case**: Creating focused datasets for specific analysis

**When to choose each approach:**
- **Use `.drop()`** when you have a large dataset and only need to remove a few problematic columns
- **Use `.filter()`** when you have many columns but only need a few for your analysis

**Question 2: Handling Missing Data**

The differences between `.dropna()` without `subset` and `.dropna(subset=['plannedShipDate', 'actualShipDate'])` are:

1. **`.dropna()` without subset:**
   - **Behavior**: Removes ANY row that has missing values in ANY column
   - **Risk**: Can remove many rows unnecessarily if other columns have missing values
   - **Use case**: When you need complete data across all columns

2. **`.dropna(subset=['plannedShipDate', 'actualShipDate'])`:**
   - **Behavior**: Only removes rows that have missing values in the specified columns
   - **Precision**: More targeted approach - only removes rows with missing critical data
   - **Use case**: When only certain columns are essential for your analysis

**Why be selective about missing value checks:**
- **Data preservation**: Avoid losing valuable data due to missing values in unimportant columns
- **Analysis focus**: Only remove rows where missing data affects your specific analysis
- **Business logic**: Some missing values might be acceptable (e.g., optional fields that don't impact shipment analysis)

### 4. Sort: Arranging Data

**Mental Model:** Order data by values or indices.

Let's sort by lateness to see the worst offenders:

```{python}
#| label: mental-model-4-sort
#| echo: true

# Sort by days late (worst first)
sorted_by_lateness = (
    clean_shipments
    .sort_values('days_late', ascending=False)  # Sort by days_late, highest first
    .reset_index(drop=True)  # Reset index to be sequential
)

print("Shipments sorted by lateness (worst first):")
print(sorted_by_lateness[['shipID', 'partID', 'days_late', 'is_late']].head(10))
```

::: {.callout-important}
## 🤔 Discussion Questions: Sort Mental Model

**Question 1: Sorting Strategies**
- What's the difference between `ascending=False` and `ascending=True` in sorting?
- How would you sort by multiple columns (e.g., first by `is_late`, then by `days_late`)?

**Question 2: Index Management**
- Why do we use `.reset_index(drop=True)` after sorting?
- What happens to the original index when you sort? Why might this be problematic?

:::

#### Briefly Give Answers to the Discussion Questions In This Section

**Question 1: Sorting Strategies**

**`ascending=False` vs `ascending=True`:**
- **`ascending=False`**: Sorts from highest to lowest values (descending order) - worst offenders first
- **`ascending=True`**: Sorts from lowest to highest values (ascending order) - best performers first

**Sorting by multiple columns:**
We can sort by multiple columns by passing a list to `sort_values()`. Here's how to sort first by `is_late`, then by `days_late`:

```python
# Sort by multiple columns
multi_sorted = (
    clean_shipments
    .sort_values(['is_late', 'days_late'], ascending=[False, False])
    .reset_index(drop=True)
)
```

**Key points:**
- **Order matters**: The first column in the list is the primary sort key
- **ascending parameter**: Can be a single boolean or a list matching the number of columns
- **Business logic**: Sort by `is_late` first (True/False), then by `days_late` within each group

**Question 2: Index Management**

**Why use `.reset_index(drop=True)` after sorting:**
- **Problem**: After sorting, the original row indices become scrambled and non-sequential
- **Solution**: `.reset_index(drop=True)` creates a new sequential index (0, 1, 2, 3...)
- **Benefit**: Makes the dataframe easier to work with and understand

**What happens to the original index when you sort:**
- **Original index**: Gets shuffled along with the data rows
- **Result**: You end up with random-looking index numbers (e.g., 45, 12, 78, 3...)
- **Problem**: This makes it confusing to reference rows by position

**Why this might be problematic:**
- **Confusion**: Hard to understand which row is which
- **Errors**: Easy to make mistakes when referencing rows by position
- **Debugging**: Makes it difficult to trace back to original data
- **Analysis**: Can cause issues with subsequent operations that rely on sequential indexing

### 5. Aggregate: Summarizing Data

**Mental Model:** Calculate summary statistics across groups or the entire dataset.

Let's calculate overall service level metrics:

```{python}
#| label: mental-model-5-aggregate
#| echo: true

# Calculate overall service level metrics
service_metrics = (
    clean_shipments
    .agg({
        'is_late': ['count', 'sum', 'mean'],  # Count total, count late, calculate percentage
        'days_late': ['mean', 'max']  # Average and maximum days late
    })
    .round(3)
)

print("Overall Service Level Metrics:")
print(service_metrics)

# Calculate percentage on-time directly from the data
on_time_rate = (1 - clean_shipments['is_late'].mean()) * 100
print(f"\nOn-time delivery rate: {on_time_rate:.1f}%")
```

::: {.callout-important}
## 🤔 Discussion Questions: Aggregate Mental Model

**Question 1: Boolean Aggregation**
- Why does `sum()` work on boolean values? What does it count?

:::

#### Briefly Give Answers to the Discussion Questions In This Section

**Question 1: Boolean Aggregation**

**Why does `sum()` work on boolean values?**

In Python and pandas, boolean values have numeric representations:
- **`True` = 1**
- **`False` = 0**

When you call `.sum()` on a boolean series, it treats `True` as 1 and `False` as 0, then adds them up.

**What does it count?**

The `sum()` function on boolean values counts the **number of `True` values** in the series.

**Practical Example:**

```python
# Example boolean series
is_late_series = pd.Series([True, False, True, True, False, True])

# What sum() does:
print(is_late_series.sum())  # Output: 4
# This counts: True(1) + False(0) + True(1) + True(1) + False(0) + True(1) = 4

# What mean() does:
print(is_late_series.mean())  # Output: 0.6667 (4/6 = 66.67%)
# This calculates the proportion of True values
```

**In the context of your shipment data:**
- **`is_late.sum()`** = Count of late shipments
- **`is_late.mean()`** = Proportion/percentage of late shipments  
- **`is_late.count()`** = Total number of shipments (including both late and on-time)

**Why this is powerful:**
- **Counting**: `sum()` gives you the count of True values
- **Proportions**: `mean()` gives you the percentage/proportion
- **Business metrics**: Perfect for calculating success rates, failure rates, etc.

### 6. Merge: Combining Information

**Mental Model:** Join data from multiple sources to create richer datasets.

Now let's analyze service levels by product category. First, we need to merge our data:

```{python}
#| label: mental-model-6-merge-prep
#| echo: true

# Merge shipment data with product line data
shipments_with_category = (
    clean_shipments
    .merge(product_line_df, on='partID', how='left')  # Left join to keep all shipments
    .assign(
        category_late=lambda df: df['is_late'] & df['prodCategory'].notna()  # Only count as late if we have category info
    )
)

print("\nProduct categories available:")
print(shipments_with_category['prodCategory'].value_counts())
```

::: {.callout-important}
## 🤔 Discussion Questions: Merge Mental Model

**Question 1: Join Types and Data Loss**
- Why does your professor think we should use `how='left'` in most cases? 
- How can you check if any shipments were lost during the merge?

**Question 2: Key Column Matching**
- What happens if there are duplicate `partID` values in the `product_line_df`?

:::

#### Briefly Give Answers to the Discussion Questions In This Section

**Question 1: Join Types and Data Loss**

**Why use `how='left'` in most cases?**

The `how='left'` join preserves **all rows from the left dataframe** (your main dataset) while adding matching information from the right dataframe. This is important because:

1. **Data Preservation**: You don't lose any shipments from your main dataset
2. **Business Logic**: Every shipment should be analyzed, even if product category info is missing
3. **Analysis Completeness**: Missing category info doesn't invalidate the shipment data
4. **Error Detection**: You can identify which shipments lack category information

**How to check if any shipments were lost during the merge:**

```python
# Check for data loss
original_count = len(clean_shipments)
merged_count = len(shipments_with_category)
lost_shipments = original_count - merged_count

print(f"Original shipments: {original_count}")
print(f"After merge: {merged_count}")
print(f"Lost shipments: {lost_shipments}")

# Check for missing category data
missing_category = shipments_with_category['prodCategory'].isna().sum()
print(f"Shipments with missing category: {missing_category}")
```

**Question 2: Key Column Matching**

**What happens if there are duplicate `partID` values in `product_line_df`?**

This creates a **many-to-many relationship** that can cause serious problems:

1. **Data Multiplication**: Each shipment with a duplicate `partID` will be matched with ALL duplicate entries
2. **Inflated Counts**: Your analysis will show artificially high numbers
3. **Incorrect Metrics**: Service level calculations will be wrong
4. **Memory Issues**: Large datasets can become exponentially larger

**Example of the problem:**
```python
# If product_line_df has duplicate partID 'ABC123':
# partID    productLine    prodCategory
# ABC123   Electronics    Electronics  
# ABC123   Electronics    Electronics  (duplicate!)

# Every shipment with partID 'ABC123' will appear twice in merged data
```

**How to detect and fix:**
```python
# Check for duplicates before merging
duplicates = product_line_df['partID'].duplicated().sum()
print(f"Duplicate partIDs in product_line_df: {duplicates}")

# Remove duplicates if found
product_line_df_clean = product_line_df.drop_duplicates(subset=['partID'])
```

### 7. Split-Apply-Combine: Group Analysis

**Mental Model:** Group data and apply functions within each group.

Now let's analyze service levels by category:

```{python}
#| label: mental-model-7-groupby
#| echo: true

# Analyze service levels by product category
service_by_category = (
    shipments_with_category
    .groupby('prodCategory')  # Split by product category
    .agg({
        'is_late': ['any', 'count', 'sum', 'mean'],  # Count, late count, percentage late
        'days_late': ['mean', 'max']  # Average and max days late
    })
    .round(3)
)

print("Service Level by Product Category:")
print(service_by_category)
```

::: {.callout-important}
## 🤔 Discussion Questions: Split-Apply-Combine Mental Model

**Question 1: GroupBy Mechanics**
- What does `.groupby('prodCategory')` actually do? How does it "split" the data?
- Why do we need to use `.agg()` after grouping? What happens if you don't?

**Question 2: Multi-Level Grouping**
- Explore grouping by `['shipID', 'prodCategory']`?  What question does this answer versus grouping by `'prodCategory'` alone?  (HINT: There may be many rows with identical shipID's due to a particular order having multiple partID's.)
:::

#### Briefly Give Answers to the Discussion Questions In This Section

**Question 1: GroupBy Mechanics**

**What does `.groupby('prodCategory')` actually do?**

The `.groupby('prodCategory')` operation performs a **three-step process**:

1. **Split**: Divides the dataframe into separate groups based on unique values in `prodCategory`
2. **Apply**: Applies the specified function(s) to each group independently  
3. **Combine**: Merges the results back into a single dataframe

**How does it "split" the data?**

```python
# Conceptual example of what groupby does:
# Original data:
# shipID  partID  prodCategory  is_late
# 001     A123    Electronics   True
# 002     B456    Clothing      False  
# 003     C789    Electronics   True
# 004     D012    Clothing      True

# After groupby('prodCategory') - splits into:
# Group 1 (Electronics):
# 001     A123    Electronics   True
# 003     C789    Electronics   True

# Group 2 (Clothing):
# 002     B456    Clothing      False
# 004     D012    Clothing      True
```

**Why do we need `.agg()` after grouping?**

- **Without `.agg()`**: You get a GroupBy object that can't be displayed or analyzed
- **With `.agg()`**: You specify what calculations to perform on each group
- **Result**: Transforms the grouped data into summary statistics

**What happens if you don't use `.agg()`?**
```python
# This creates a GroupBy object (not useful for analysis)
grouped = shipments_with_category.groupby('prodCategory')
print(type(grouped))  # <class 'pandas.core.groupby.DataFrameGroupBy'>

# You need .agg() to get actual results
results = grouped.agg({'is_late': 'mean'})  # Now you get summary statistics
```

**Question 2: Multi-Level Grouping**

**Grouping by `['shipID', 'prodCategory']` vs `'prodCategory'` alone:**

**Single-level grouping (`'prodCategory'`):**
- **Question answered**: "What's the overall service level for each product category?"
- **Analysis level**: Category-level summary
- **Example result**: Electronics: 75% on-time, Clothing: 80% on-time

**Multi-level grouping (`['shipID', 'prodCategory']`):**
- **Question answered**: "For each shipment, what's the service level by category within that shipment?"
- **Analysis level**: Shipment-category level (more granular)
- **Business insight**: Identifies shipments that span multiple categories

**Why this matters for business analysis:**

```python
# Multi-level grouping reveals:
# shipID  prodCategory  is_late
# 001     Electronics   True
# 001     Clothing      False  # Same shipment, different category performance

# This answers: "Do shipments with multiple categories have different 
# service levels across categories within the same order?"
```

**Key insight**: Multi-level grouping helps identify **cross-category shipment patterns** - crucial for understanding if certain product combinations affect service levels.

## Answering A Business Question

**Mental Model:** Combine multiple data manipulation techniques to answer complex business questions.

Let's create a comprehensive analysis by combining shipment-level data with category information:

```{python}
#| label: mental-model-7-comprehensive
#| echo: true

# Create a comprehensive analysis dataset
comprehensive_analysis = (
    shipments_with_category
    .groupby(['shipID', 'prodCategory'])  # Group by shipment and category
    .agg({
        'is_late': 'any',  # True if any item in this shipment/category is late
        'days_late': 'max'  # Maximum days late for this shipment/category
    })
    .reset_index()
    .assign(
        has_multiple_categories=lambda df: df.groupby('shipID')['prodCategory'].transform('nunique') > 1
    )
)

print("Comprehensive analysis - shipments with multiple categories:")
multi_category_shipments = comprehensive_analysis[comprehensive_analysis['has_multiple_categories']]
print(f"Shipments with multiple categories: {multi_category_shipments['shipID'].nunique()}")
print(f"Total unique shipments: {comprehensive_analysis['shipID'].nunique()}")
print(f"Percentage with multiple categories: {multi_category_shipments['shipID'].nunique() / comprehensive_analysis['shipID'].nunique() * 100:.1f}%")
```

::: {.callout-important}
## 🤔 Discussion Questions: Answering A Business Question

**Question 1: Business Question Analysis**
- What business question does this comprehensive analysis answer?
- How does grouping by `['shipID', 'prodCategory']` differ from grouping by just `'prodCategory'`?
- What insights can ZappTech's management gain from knowing the percentage of multi-category shipments?
:::

#### Briefly Give Answers to the Discussion Questions In This Section

**Question 1: Business Question Analysis**

**What business question does this comprehensive analysis answer?**

This analysis answers the critical business question: **"How often do customers order products from multiple categories in a single shipment, and what are the service level implications?"**

**Key business insights:**
- **Cross-category shopping patterns**: Understanding customer behavior
- **Operational complexity**: Multi-category shipments may be harder to fulfill
- **Service level impact**: Whether mixed-category orders affect delivery performance
- **Inventory management**: Implications for warehouse operations

**How does grouping by `['shipID', 'prodCategory']` differ from grouping by just `'prodCategory'`?**

**Single-level grouping (`'prodCategory'`):**
- **Question**: "What's the overall service level for each product category?"
- **Analysis**: Category-level summary across all shipments
- **Result**: Electronics: 75% on-time, Clothing: 80% on-time

**Multi-level grouping (`['shipID', 'prodCategory']`):**
- **Question**: "For each individual shipment, what's the service level by category within that specific shipment?"
- **Analysis**: Shipment-category level (more granular)
- **Result**: Identifies shipments that span multiple categories and their performance

**Key difference**: Multi-level grouping reveals **within-shipment patterns** - whether the same shipment performs differently across categories.

**What insights can ZappTech's management gain from knowing the percentage of multi-category shipments?**

**Strategic Insights:**
1. **Customer Behavior**: Understanding cross-category shopping patterns
2. **Operational Planning**: Multi-category shipments may require different fulfillment processes
3. **Service Level Management**: Mixed orders might have different performance characteristics
4. **Inventory Strategy**: Warehouse layout and picking processes optimization

**Actionable Business Decisions:**
- **Fulfillment Process**: Should multi-category orders be handled differently?
- **Service Level Targets**: Do mixed orders need different performance standards?
- **Customer Experience**: Are customers satisfied with mixed-category delivery times?
- **Operational Efficiency**: Can warehouse processes be optimized for mixed orders?

**Example Business Impact:**
```python
# If 30% of shipments are multi-category:
# Management might decide to:
# - Create specialized fulfillment teams for mixed orders
# - Set different service level targets for multi-category shipments
# - Optimize warehouse layout to reduce picking time for mixed orders
# - Develop customer communication strategies for mixed-order delays
```

## Student Analysis Section: Mastering Data Manipulation {#student-analysis-section}

**Your Task:** Demonstrate your mastery of the seven mental models through comprehensive discussion and analysis. The bulk of your grade comes from thoughtfully answering the discussion questions for each mental model. See below for more details.

### Core Challenge: Discussion Questions Analysis

**For each mental model, provide:**
- Clear, concise answers to all discussion questions
- Code examples where appropriate to support your explanations

::: {.callout-important}
## 📊 Discussion Questions Requirements

**Complete all discussion question sections:**
1. **Assign Mental Model:** Data types, date handling, and debugging
2. **Subset Mental Model:** Filtering strategies and complex queries
3. **Drop Mental Model:** Data cleaning and quality management
4. **Sort Mental Model:** Data organization and business logic
5. **Aggregate Mental Model:** Summary statistics and business metrics
6. **Merge Mental Model:** Data integration and quality control
7. **Split-Apply-Combine Mental Model:** Group analysis and advanced operations
8. **Answering A Business Question:** Combining multiple data manipulation techniques to answer a business question
:::

### Professional Visualizations (For 100% Grade)

**Your Task:** Create a professional visualization that supports your analysis and demonstrates your understanding of the data.

**Create visualizations showing:**
- Service level (on-time percentage) by product category

**Your visualizations should:**
- Use clear labels and professional formatting
- Support the insights from your discussion questions
- Be appropriate for a business audience
- Do not `echo` the code that creates the visualizations

## Professional Data Visualizations

The following visualizations provide comprehensive insights into ZappTech's service level performance across product categories, supporting the business analysis from our discussion questions.

```{python}
#| label: professional-visualizations
#| echo: false
#| fig-cap: "Service Level Analysis by Product Category"
#| fig-subcap: 
#|   - "On-Time Delivery Rate by Category"
#|   - "Average Days Late by Category"
#|   - "Shipment Volume and Performance Overview"
#| layout-ncol: 2

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Set professional styling
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")

# Prepare data for visualization
viz_data = (
    shipments_with_category
    .groupby('prodCategory')
    .agg({
        'is_late': ['count', 'sum', 'mean'],
        'days_late': ['mean', 'max']
    })
    .round(3)
)

# Flatten column names for easier access
viz_data.columns = ['_'.join(col).strip() for col in viz_data.columns]
viz_data = viz_data.reset_index()

# Calculate on-time percentage
viz_data['on_time_percentage'] = (1 - viz_data['is_late_mean']) * 100
viz_data['total_shipments'] = viz_data['is_late_count']
viz_data['late_shipments'] = viz_data['is_late_sum']

# Sort by on-time percentage for better visualization
viz_data = viz_data.sort_values('on_time_percentage', ascending=True)

# Create the first visualization: On-Time Delivery Rate by Category
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))

# Bar chart for on-time percentage
bars1 = ax1.barh(viz_data['prodCategory'], viz_data['on_time_percentage'], 
                 color=['#2E8B57' if x >= 80 else '#FF6B6B' if x < 70 else '#FFA500' 
                        for x in viz_data['on_time_percentage']])

ax1.set_xlabel('On-Time Delivery Rate (%)', fontsize=12, fontweight='bold')
ax1.set_ylabel('Product Category', fontsize=12, fontweight='bold')
ax1.set_title('Service Level Performance by Product Category\n(On-Time Delivery Rate)', 
              fontsize=14, fontweight='bold', pad=20)

# Add value labels on bars
for i, (bar, value) in enumerate(zip(bars1, viz_data['on_time_percentage'])):
    ax1.text(value + 1, i, f'{value:.1f}%', va='center', fontweight='bold')

# Add performance thresholds
ax1.axvline(x=80, color='green', linestyle='--', alpha=0.7, label='Target (80%)')
ax1.axvline(x=70, color='orange', linestyle='--', alpha=0.7, label='Warning (70%)')
ax1.legend()

# Second visualization: Average Days Late
bars2 = ax2.barh(viz_data['prodCategory'], viz_data['days_late_mean'], 
                 color=['#FF6B6B' if x > 5 else '#FFA500' if x > 2 else '#2E8B57' 
                        for x in viz_data['days_late_mean']])

ax2.set_xlabel('Average Days Late', fontsize=12, fontweight='bold')
ax2.set_ylabel('Product Category', fontsize=12, fontweight='bold')
ax2.set_title('Average Delay by Product Category\n(Days Late)', 
              fontsize=14, fontweight='bold', pad=20)

# Add value labels on bars
for i, (bar, value) in enumerate(zip(bars2, viz_data['days_late_mean'])):
    ax2.text(value + 0.1, i, f'{value:.1f}', va='center', fontweight='bold')

# Add performance thresholds
ax2.axvline(x=2, color='green', linestyle='--', alpha=0.7, label='Good (<2 days)')
ax2.axvline(x=5, color='orange', linestyle='--', alpha=0.7, label='Concerning (>5 days)')
ax2.legend()

plt.tight_layout()
plt.show()

# Create a comprehensive overview visualization
fig, ax = plt.subplots(figsize=(14, 10))

# Create a scatter plot with bubble sizes representing shipment volume
scatter = ax.scatter(viz_data['on_time_percentage'], viz_data['days_late_mean'], 
                    s=viz_data['total_shipments']*3, alpha=0.7, 
                    c=viz_data['on_time_percentage'], cmap='RdYlGn', 
                    edgecolors='black', linewidth=1.5)

# Add category labels
for i, category in enumerate(viz_data['prodCategory']):
    ax.annotate(category, 
                (viz_data['on_time_percentage'].iloc[i], viz_data['days_late_mean'].iloc[i]),
                xytext=(5, 5), textcoords='offset points', fontweight='bold')

# Add performance quadrants
ax.axhline(y=2, color='gray', linestyle='--', alpha=0.5)
ax.axvline(x=80, color='gray', linestyle='--', alpha=0.5)

# Add quadrant labels
ax.text(85, 1, 'High Performance\nZone', fontsize=10, ha='center', 
        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen", alpha=0.7))
ax.text(75, 6, 'Needs Attention\nZone', fontsize=10, ha='center',
        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightcoral", alpha=0.7))

ax.set_xlabel('On-Time Delivery Rate (%)', fontsize=12, fontweight='bold')
ax.set_ylabel('Average Days Late', fontsize=12, fontweight='bold')
ax.set_title('Service Level Performance Overview\n(Bubble size = Shipment Volume)', 
             fontsize=14, fontweight='bold', pad=20)

# Add colorbar
cbar = plt.colorbar(scatter, ax=ax)
cbar.set_label('On-Time Rate (%)', fontsize=10, fontweight='bold')

# Add legend for bubble sizes
legend_elements = [
    plt.scatter([], [], s=100, c='gray', alpha=0.7, label='Small Volume'),
    plt.scatter([], [], s=300, c='gray', alpha=0.7, label='Medium Volume'),
    plt.scatter([], [], s=500, c='gray', alpha=0.7, label='Large Volume')
]
ax.legend(handles=legend_elements, title='Shipment Volume', loc='upper right')

plt.tight_layout()
plt.show()

# Create a summary table for business presentation
print("="*80)
print("ZAPPTECH SERVICE LEVEL PERFORMANCE SUMMARY")
print("="*80)
print(f"{'Category':<20} {'On-Time %':<12} {'Avg Days Late':<15} {'Total Shipments':<15}")
print("-"*80)

for _, row in viz_data.iterrows():
    performance_status = "🟢 Excellent" if row['on_time_percentage'] >= 85 else \
                        "🟡 Good" if row['on_time_percentage'] >= 75 else \
                        "🟠 Needs Improvement" if row['on_time_percentage'] >= 65 else "🔴 Critical"
    
    print(f"{row['prodCategory']:<20} {row['on_time_percentage']:<11.1f}% {row['days_late_mean']:<14.1f} {row['total_shipments']:<15}")
    print(f"{'':20} {performance_status:<12} {'':15} {'':15}")

print("-"*80)
print(f"{'OVERALL':<20} {(1-shipments_with_category['is_late'].mean())*100:<11.1f}% {shipments_with_category['days_late'].mean():<14.1f} {len(shipments_with_category):<15}")
print("="*80)
```

## Key Business Insights from Visualizations

### Performance Analysis
- **High Performers**: Categories with >85% on-time rate and <2 days average delay
- **Attention Needed**: Categories with <75% on-time rate or >5 days average delay
- **Volume Impact**: Larger shipment volumes may indicate operational challenges

### Strategic Recommendations
1. **Immediate Action**: Focus on categories in the "Needs Attention" zone
2. **Process Improvement**: Investigate operational differences between high and low performers
3. **Resource Allocation**: Consider additional resources for high-volume, low-performance categories
4. **Customer Communication**: Develop targeted communication strategies for categories with delays

## Challenge Requirements 📋

**Your Primary Task:** Answer all discussion questions for the seven mental models with thoughtful, well-reasoned responses that demonstrate your understanding of data manipulation concepts.

**Key Requirements:**
- Complete discussion questions for each mental model
- Demonstrate clear understanding of pandas concepts and data manipulation techniques
- Write clear, business-focused analysis that explains your findings

## Getting Started: Repository Setup 🚀

::: {.callout-important}
## 📁 Getting Started

**Step 1:** Fork and clone this challenge repository
- Go to the course repository and find the "dataManipulationChallenge" folder
- Fork it to your GitHub account, or clone it directly
- Open the cloned repository in Cursor

**Step 2:** Set up your Python environment
- Follow the Python setup instructions above (use your existing venv from Tech Setup Challenge Part 2)
- Make sure your virtual environment is activated and the Python interpreter is set

**Step 3:** You're ready to start! The data loading code is already provided in this file.

**Note:** This challenge uses the same `index.qmd` file you're reading right now - you'll edit it to complete your analysis.
:::


### Getting Started Tips

::: {.callout-note}
## 🎯 Method Chaining Philosophy

> "Each operation should build naturally on the previous one"

*Think of method chaining like building with LEGO blocks - each piece connects to the next, creating something more complex and useful than the individual pieces.*
:::

::: {.callout-warning}
## 💾 Important: Save Your Work Frequently!

**Before you start:** Make sure to commit your work often using the Source Control panel in Cursor (Ctrl+Shift+G or Cmd+Shift+G). This prevents the AI from overwriting your progress and ensures you don't lose your work.

**Commit after each major step:**

- After completing each mental model section
- After adding your visualizations
- After completing your advanced method chain
- Before asking the AI for help with new code

**How to commit:**

1. Open Source Control panel (Ctrl+Shift+G)
2. Stage your changes (+ button)
3. Write a descriptive commit message
4. Click the checkmark to commit

*Remember: Frequent commits are your safety net!*
:::

## Grading Rubric 🎓

**75% Grade:** Complete discussion questions for at least 5 of the 7 mental models with clear, thoughtful responses.

**85% Grade:** Complete discussion questions for all 7 mental models with comprehensive, well-reasoned responses.

**95% Grade:** Complete all discussion questions plus the "Answering A Business Question" section.

**100% Grade:** Complete all discussion questions plus create a professional visualization showing service level by product category.

## Submission Checklist ✅

**Minimum Requirements (Required for Any Points):**

- [ ] Created repository named "dataManipulationChallenge" in your GitHub account
- [ ] Cloned repository locally using Cursor (or VS Code)
- [ ] Completed discussion questions for at least 5 of the 7 mental models
- [ ] Document rendered to HTML successfully
- [ ] HTML files uploaded to your repository
- [ ] GitHub Pages enabled and working
- [ ] Site accessible at `https://[your-username].github.io/dataManipulationChallenge/`

**75% Grade Requirements:**

- [ ] Complete discussion questions for at least 5 of the 7 mental models
- [ ] Clear, thoughtful responses that demonstrate understanding
- [ ] Code examples where appropriate to support explanations

**85% Grade Requirements:**

- [ ] Complete discussion questions for all 7 mental models
- [ ] Comprehensive, well-reasoned responses showing deep understanding
- [ ] Business context for why concepts matter
- [ ] Examples of real-world applications

**95% Grade Requirements:**

- [ ] Complete discussion questions for all 7 mental models
- [ ] Complete the "Answering A Business Question" discussion questions
- [ ] Comprehensive, well-reasoned responses showing deep understanding
- [ ] Business context for why concepts matter

**100% Grade Requirements:**

- [ ] All discussion questions completed with professional quality
- [ ] Professional visualization showing service level by product category
- [ ] Professional presentation style appropriate for business audience
- [ ] Clear, engaging narrative that tells a compelling story
- [ ] Practical insights that would help ZappTech's management

**Report Quality (Critical for Higher Grades):**

- [ ] Professional writing style (no AI-generated fluff)
- [ ] Concise analysis that gets to the point


